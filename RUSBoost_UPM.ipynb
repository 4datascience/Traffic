{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uZ_KQy9CaghT",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **ONLY YEAR 2017, Replica of RUSBoost previous work**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "J1aLPKnRdfSq"
   },
   "source": [
    "## Python Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2O_6vDSUdG5G"
   },
   "source": [
    "### Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aZLKV7udGBe"
   },
   "outputs": [],
   "source": [
    "# Data libraries\n",
    "#from google.colab import drive\n",
    "import os\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from seaborn import violinplot, boxenplot\n",
    "\n",
    "# Model training\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import base\n",
    "import joblib\n",
    "    # Keras\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation, Lambda\n",
    "    # Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Statistical tests\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4sHUoA4jdmit"
   },
   "source": [
    "### SET ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGdil-f9dr5k"
   },
   "outputs": [],
   "source": [
    "# WORKING FOLDER: /content/local/Traffic/data or /content/drive/MyDrive/Traffic/data\n",
    "ROOT = '/content/local/Traffic/data'\n",
    "# PREDICTION HORIZON\n",
    "PREDICTED_MINUTES_AHEAD = 5\n",
    "if (PREDICTED_MINUTES_AHEAD%5 != 0): raise ValueError(\"Invalid prediction horizon. Must be multiple of 5 minutes\")\n",
    "INPUT_MINUTES_BEFORE = 45\n",
    "if (INPUT_MINUTES_BEFORE%5 != 0): raise ValueError(\"Invalid input minutes span. Must be multiple of 5 minutes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wOQaX0TnQJti"
   },
   "source": [
    "# **LOAD DATA**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fRqNJgJxQf8l",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DATA [ LINK_ID, TT_ARR, LOS_DEP ] (Year 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Jet-m9yQJSQ",
    "outputId": "8b659314-fbf3-4f74-ba1d-c4af2da2e837"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_mon = []\n",
    "training_week = []\n",
    "training_fri = []\n",
    "training_weeknd = []\n",
    "\n",
    "directory = '/content/drive/MyDrive/Traffic/data/processed/'\n",
    "for filename in os.listdir(directory):\n",
    "    if \"2017\" not in filename:\n",
    "        continue\n",
    "    f = os.path.join(directory, filename)\n",
    "    df=pd.read_csv(f)\n",
    "    print(f'{filename} {df.shape[0]/(60*24*6)}')\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format='%d-%b-%Y %H:%M:%S')\n",
    "    df = df.set_index(\"date\")\n",
    "    Y = df.shift(-PREDICTED_MINUTES_AHEAD, freq='min')[['link_id', 'LOS_dep']]\n",
    "    # Training set contains all data\n",
    "    X = df[['link_id', 'tt_arr']]\n",
    "    for weekday in range(0,7):\n",
    "        # Mon\n",
    "        if weekday == 0:\n",
    "            training_mon.append( pd.merge(X, Y[(Y.index.weekday == weekday)], on=['date', 'link_id'], how='inner') )\n",
    "        # Tue-Wed-Thu\n",
    "        elif weekday in [1,2,3]:\n",
    "            training_week.append( pd.merge(X, Y[(Y.index.weekday == weekday)], on=['date', 'link_id'], how='inner') )\n",
    "        elif weekday == 4:\n",
    "            training_fri.append( pd.merge(X, Y[(Y.index.weekday == weekday)], on=['date', 'link_id'], how='inner') )\n",
    "        elif weekday in [5,6]:\n",
    "            training_weeknd.append( pd.merge(X, Y[(Y.index.weekday == weekday)], on=['date', 'link_id'], how='inner') )\n",
    "\n",
    "del df,X,Y\n",
    "training_df_mon = pd.concat(training_mon)\n",
    "training_df_week = pd.concat(training_week)\n",
    "training_df_fri = pd.concat(training_fri)\n",
    "training_df_weeknd = pd.concat(training_weeknd)\n",
    "\n",
    "del training_mon,training_week,training_fri,training_weeknd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uNktJwINXeVt",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DATA [ LINK_ID, TT_ARR, TT_ARR-5, TT_ARR-10, ..., TT_ARR-N, LOS_DEP ] (Year 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BokEgQrdavRt",
    "outputId": "393ac4b5-488f-4292-a764-24ca40a30e8d"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training = []\n",
    "\n",
    "directory = '/content/drive/MyDrive/Traffic/data/processed/'\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    df=pd.read_csv(f)\n",
    "    print(f'{filename} {df.shape[0]/(60*24*6)}')\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format='%d-%b-%Y %H:%M:%S')\n",
    "    df = df.set_index(\"date\")\n",
    "    Y = df.shift(-PREDICTED_MINUTES_AHEAD, freq='min')[['link_id', 'LOS_dep']]\n",
    "    # Training set\n",
    "    X = df[['link_id', 'tt_arr']]\n",
    "    for offset in range(1,int((INPUT_MINUTES_BEFORE)/5)+1):\n",
    "        X = pd.merge(X, df.shift(offset*5, freq='min')[['link_id', 'tt_arr']], on=['date', 'link_id'], how='inner', suffixes=[None, f'-{int(offset*5)}'])\n",
    "    for weekday in range(0,7):\n",
    "        # Mon\n",
    "        if weekday == 0:\n",
    "            training_mon.append( pd.merge(X, Y[(Y.index.weekday == weekday)], on=['date', 'link_id'], how='inner') )\n",
    "        # Tue-Wed-Thu\n",
    "        elif weekday in [1,2,3]:\n",
    "            training_week.append( pd.merge(X, Y[(Y.index.weekday == weekday)], on=['date', 'link_id'], how='inner') )\n",
    "        elif weekday == 4:\n",
    "            training_fri.append( pd.merge(X, Y[(Y.index.weekday == weekday)], on=['date', 'link_id'], how='inner') )\n",
    "        elif weekday in [5,6]:\n",
    "            training_weeknd.append( pd.merge(X, Y[(Y.index.weekday == weekday)], on=['date', 'link_id'], how='inner') )\n",
    "\n",
    "del df,X,Y\n",
    "training_df_mon = pd.concat(training_mon)\n",
    "training_df_week = pd.concat(training_week)\n",
    "training_df_fri = pd.concat(training_fri)\n",
    "training_df_weeknd = pd.concat(training_weeknd)\n",
    "\n",
    "del training_mon,training_week,training_fri,training_weeknd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "56v0Qmq5YalF",
    "tags": []
   },
   "source": [
    "# **RUSBoost**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "K8YP3zubmE0w",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### *Algorithms*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ufAzojIniPjV"
   },
   "source": [
    "**Algorithm implementation M2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvGE01esiNX8"
   },
   "outputs": [],
   "source": [
    "def labelToIndex(cl,clf):\n",
    "    return clf.labelDict[cl]\n",
    "\n",
    "def indexToLabel(i,clf):\n",
    "    return clf.classes[i]\n",
    "\n",
    "class RUSBoostClassifier_:\n",
    "    \n",
    "    def __init__(self,base_estimator=None,n_estimators=50,learning_rate=1.0):\n",
    "        print(\"M2 Implementation of RUSBoost\")\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        if base_estimator == None:\n",
    "            base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "        self.base_estimator = base_estimator\n",
    "        self.estimator_errors_ = []\n",
    "        self.observation_weights_ = {}\n",
    "        \n",
    "    def classes_(self):\n",
    "        return self.classes\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        param X: DataFrame that contains the features with shape (n_samples, )\n",
    "        param y: DataFrame with the labels\n",
    "        \"\"\"        \n",
    "        \n",
    "        # Class labels mapping to indices\n",
    "        self.createLabelDict(np.unique(y))\n",
    "        k = len(self.classes)\n",
    "        # `undersampling_n` elements to sample from each class, equal #samples minority class\n",
    "        undersampling_n = min(y.value_counts())\n",
    "\n",
    "        # Initialize observation weights as 1/(N*(k-1)) where N is total `n_samples` and k is the numebr of classes\n",
    "        N = X.shape[0]\n",
    "        B = N*(k-1)\n",
    "        D = {epoch: np.full(k-1,1/B) for epoch in X.index}\n",
    "\n",
    "        # Get whole dataset samples to later calculate the weighting factors on every iteration\n",
    "        iTL = np.vectorize(labelToIndex)\n",
    "        y_indices = iTL(y.values,self)\n",
    "        \n",
    "        # M iterations (#WeakLearners)\n",
    "        for m in range(self.n_estimators):\n",
    "\n",
    "        # 1) Random UnderSampling\n",
    "            df_ = pd.DataFrame()\n",
    "            for label_ in self.classes:\n",
    "                df_ = pd.concat([ df_, X.loc[[ y==label_ ]].sample(undersampling_n, replace=False) ])\n",
    "\n",
    "            \n",
    "            # Training data initalization\n",
    "            X_ = df_.values\n",
    "            y_ = y.loc[df_.index].values\n",
    "            D_ = np.sum([D[epoch] for epoch in df_.index], axis=-1)\n",
    "\n",
    "        # 2) WeakLearner training\n",
    "            Gm = base.clone(self.base_estimator).\\\n",
    "                            fit(X_,y_,sample_weight=D_).predict_proba\n",
    "            self.models.append(Gm)\n",
    "        \n",
    "        # 3) Error-rate computation\n",
    "            predictions_proba = Gm(X.values)\n",
    "            sum_pseudolosses = 0\n",
    "            for i, epoch in enumerate(D.keys()):\n",
    "                k_index = 0\n",
    "                for cl in range(k):\n",
    "                    if cl != y_indices[i]:\n",
    "                        sum_pseudolosses += D[epoch][k_index]*(1-predictions_proba[i,y_indices[i]]+predictions_proba[i,cl])\n",
    "                        k_index += 1\n",
    "\n",
    "            error = 0.5 * sum_pseudolosses\n",
    "            self.estimator_errors_.append(error)\n",
    "        \n",
    "        # 4) WeakLearner weight for ensemble computation\n",
    "            BetaM = error/(1-error)\n",
    "            self.models[m] = (BetaM,Gm)\n",
    "\n",
    "        # 5) Observation weights update for next iteration with weights normalization\n",
    "            norm_ = 0\n",
    "            for i, epoch in enumerate(D.keys()):\n",
    "                k_index = 0\n",
    "                for cl in range(k):\n",
    "                    if cl != y_indices[i]:\n",
    "                        w_ = 0.5*(1+predictions_proba[i,y_indices[i]]-predictions_proba[i,cl])\n",
    "                        D[epoch][k_index] *= BetaM**(self.learning_rate*w_)\n",
    "                        norm_ += D[epoch][k_index]\n",
    "                        k_index += 1\n",
    "            for epoch in D.keys():\n",
    "                D[epoch] /= norm_\n",
    "        \n",
    "        self.observation_weights_ = D\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    def createLabelDict(self,classes):\n",
    "        self.labelDict = {}\n",
    "        self.classes = classes\n",
    "        for i,cl in enumerate(classes):\n",
    "            self.labelDict[cl] = i\n",
    "    \n",
    "    def predict(self,X):\n",
    "        sum_model_hypothesis = np.sum(np.stack([-np.log(Bm)*Gm(X) for Bm,Gm in self.models], axis=-1), axis=-1)\n",
    "        iTL = np.vectorize(indexToLabel)            \n",
    "        return iTL(np.argmax(sum_model_hypothesis,axis=1),self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for LINK in range(5,11):\n",
    "    print(f\"LINK {LINK}\")\n",
    "    other_links = [5,6,7,8,9,10]; other_links.remove(LINK)\n",
    "    training_df_link = training_df.copy()\n",
    "    training_df_link = training_df_link[training_df_link['link_id'] == LINK]\n",
    "\n",
    "    for other_link in other_links:\n",
    "        training_df_link = pd.merge(training_df_link, training_df[training_df['link_id'] == other_link].filter(regex=(\"tt_arr.*\")), on=['date'], how='inner', suffixes=[None, f'_{other_link}'])\n",
    "    \n",
    "    X = training_df_link.filter(regex=(\"tt_arr.*\")).values\n",
    "    y = training_df_link['LOS_dep'].values\n",
    "    clf = make_pipeline(preprocessing.StandardScaler(), GaussianNB())\n",
    "\n",
    "    # 5-fold cross validation\n",
    "    CV = 5\n",
    "    scoring = {'recall1': metrics.make_scorer(metrics.recall_score, average = None, labels = [1]), \n",
    "        'recall2': metrics.make_scorer(metrics.recall_score, average = None, labels = [2]),\n",
    "        'recall3': metrics.make_scorer(metrics.recall_score, average = None, labels = [3]),\n",
    "        'recall4': metrics.make_scorer(metrics.recall_score, average = None, labels = [4]), \n",
    "        'recall5': metrics.make_scorer(metrics.recall_score, average = None, labels = [5]),\n",
    "        'recall6': metrics.make_scorer(metrics.recall_score, average = None, labels = [6])}\n",
    "    results = cross_validate(clf, X, y, cv=CV, scoring=scoring, return_train_score = False)\n",
    "    for LOS in range(1,7):\n",
    "        score = results[f'test_recall{LOS}']\n",
    "        print(f\"\\tLOS {LOS}: Recall {round(sum(score)/CV*100, 2)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8T8KoJtliq5",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **NO FEATURE ENGINEERING M2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqF7lfLMlpOp",
    "outputId": "61226b7f-eda0-44bc-9116-6a4b60a97e49"
   },
   "outputs": [],
   "source": [
    "for LINK in range(5,11):\n",
    "    print(f\"LINK {LINK}\")\n",
    "\n",
    "    features_name = 'tt_arr$'\n",
    "\n",
    "    print(f\"MONDAY\")\n",
    "    model = RUSBoostClassifier_(base_estimator=DecisionTreeClassifier(max_depth=128), n_estimators=30, learning_rate=0.3)\n",
    "    X = training_df_mon[training_df_mon['link_id'] == LINK].filter(regex=(features_name))\n",
    "    y = training_df_mon[training_df_mon['link_id'] == LINK]['LOS_dep']\n",
    "    # 5-fold cross validation\n",
    "    CV = 5\n",
    "    scoring = {'recall1': metrics.make_scorer(metrics.recall_score, average = None, labels = [1]), \n",
    "        'recall2': metrics.make_scorer(metrics.recall_score, average = None, labels = [2]),\n",
    "        'recall3': metrics.make_scorer(metrics.recall_score, average = None, labels = [3]),\n",
    "        'recall4': metrics.make_scorer(metrics.recall_score, average = None, labels = [4]), \n",
    "        'recall5': metrics.make_scorer(metrics.recall_score, average = None, labels = [5]),\n",
    "        'recall6': metrics.make_scorer(metrics.recall_score, average = None, labels = [6])}\n",
    "    results = cross_validate(model, X, y, cv=CV, scoring=scoring, return_train_score = False)\n",
    "    for LOS in range(1,7):\n",
    "        score = results[f'test_recall{LOS}']\n",
    "        print(f\"\\tLOS {LOS}: Recall {round(sum(score)/CV*100, 2)}%\")\n",
    "    \n",
    "    print(f\"TUESDAY-WEDNESDAY-THURSDAY\")\n",
    "    model = RUSBoostClassifier_(base_estimator=DecisionTreeClassifier(max_depth=196), n_estimators=30, learning_rate=0.3)\n",
    "    X = training_df_week[training_df_week['link_id'] == LINK].filter(regex=(features_name))\n",
    "    y = training_df_week[training_df_week['link_id'] == LINK]['LOS_dep']\n",
    "    # 5-fold cross validation\n",
    "    CV = 5\n",
    "    scoring = {'recall1': metrics.make_scorer(metrics.recall_score, average = None, labels = [1]), \n",
    "        'recall2': metrics.make_scorer(metrics.recall_score, average = None, labels = [2]),\n",
    "        'recall3': metrics.make_scorer(metrics.recall_score, average = None, labels = [3]),\n",
    "        'recall4': metrics.make_scorer(metrics.recall_score, average = None, labels = [4]), \n",
    "        'recall5': metrics.make_scorer(metrics.recall_score, average = None, labels = [5]),\n",
    "        'recall6': metrics.make_scorer(metrics.recall_score, average = None, labels = [6])}\n",
    "    results = cross_validate(model, X, y, cv=CV, scoring=scoring, return_train_score = False)\n",
    "    for LOS in range(1,7):\n",
    "        score = results[f'test_recall{LOS}']\n",
    "        print(f\"\\tLOS {LOS}: Recall {round(sum(score)/CV*100, 2)}%\")\n",
    "\n",
    "    print(f\"FRIDAY\")\n",
    "    model = RUSBoostClassifier_(base_estimator=DecisionTreeClassifier(max_depth=128), n_estimators=30, learning_rate=0.3)\n",
    "    X = training_df_fri[training_df_fri['link_id'] == LINK].filter(regex=(features_name))\n",
    "    y = training_df_fri[training_df_fri['link_id'] == LINK]['LOS_dep']\n",
    "    # 5-fold cross validation\n",
    "    CV = 5\n",
    "    scoring = {'recall1': metrics.make_scorer(metrics.recall_score, average = None, labels = [1]), \n",
    "        'recall2': metrics.make_scorer(metrics.recall_score, average = None, labels = [2]),\n",
    "        'recall3': metrics.make_scorer(metrics.recall_score, average = None, labels = [3]),\n",
    "        'recall4': metrics.make_scorer(metrics.recall_score, average = None, labels = [4]), \n",
    "        'recall5': metrics.make_scorer(metrics.recall_score, average = None, labels = [5]),\n",
    "        'recall6': metrics.make_scorer(metrics.recall_score, average = None, labels = [6])}\n",
    "    results = cross_validate(model, X, y, cv=CV, scoring=scoring, return_train_score = False)\n",
    "    for LOS in range(1,7):\n",
    "        score = results[f'test_recall{LOS}']\n",
    "        print(f\"\\tLOS {LOS}: Recall {round(sum(score)/CV*100, 2)}%\")\n",
    "\n",
    "    print(f\"WEEKEND\")\n",
    "    model = RUSBoostClassifier_(base_estimator=DecisionTreeClassifier(max_depth=128), n_estimators=30, learning_rate=0.3)\n",
    "    X = training_df_weeknd[training_df_weeknd['link_id'] == LINK].filter(regex=(features_name))\n",
    "    y = training_df_weeknd[training_df_weeknd['link_id'] == LINK]['LOS_dep']\n",
    "    # 5-fold cross validation\n",
    "    CV = 5\n",
    "    scoring = {'recall1': metrics.make_scorer(metrics.recall_score, average = None, labels = [1]), \n",
    "        'recall2': metrics.make_scorer(metrics.recall_score, average = None, labels = [2]),\n",
    "        'recall3': metrics.make_scorer(metrics.recall_score, average = None, labels = [3]),\n",
    "        'recall4': metrics.make_scorer(metrics.recall_score, average = None, labels = [4]), \n",
    "        'recall5': metrics.make_scorer(metrics.recall_score, average = None, labels = [5]),\n",
    "        'recall6': metrics.make_scorer(metrics.recall_score, average = None, labels = [6])}\n",
    "    results = cross_validate(model, X, y, cv=CV, scoring=scoring, return_train_score = False)\n",
    "    for LOS in range(1,7):\n",
    "        score = results[f'test_recall{LOS}']\n",
    "        print(f\"\\tLOS {LOS}: Recall {round(sum(score)/CV*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "x6zbR-vhlrsx",
    "outputId": "4cc16337-c95d-44de-dbe4-1562486d8c42"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(model.n_estimators), model.estimator_errors_)\n",
    "\n",
    "ax.set(xlabel='#Estimator', ylabel='Estimation error',\n",
    "       title='Evolution of errors training estimators')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9Dm-Rr2LryUa",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Feature engineering SAMME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5GwdnL80wEtn"
   },
   "source": [
    "### **FEATURE ENGINEERING ADJACENT LINK INFORMATION**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SX6uOIxdwEtp"
   },
   "source": [
    "*Features creation*\n",
    "\n",
    "For each link, the tt_arrival of the other links are added as features. Therefore we will be using 6 features in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lG3lIJ4Swav-"
   },
   "outputs": [],
   "source": [
    "for LINK in range(5,11):\n",
    "    print(f\"LINK {LINK}\")\n",
    "    filename = f'{ROOT}/models/RUSBoost_{PREDICTED_MINUTES_AHEAD}m_SAMME_adjacent_link{LINK}.joblib'\n",
    "\n",
    "    other_links = [5,6,7,8,9,10]; other_links.remove(LINK)\n",
    "    training_df_link = training_df.copy()\n",
    "    training_df_link = training_df_link[training_df_link['link_id'] == LINK]\n",
    "\n",
    "    for other_link in other_links:\n",
    "        training_df_link = pd.merge(training_df_link, training_df[training_df['link_id'] == other_link].filter(regex=(\"tt_ar[^-]+$\")), on=['date'], how='inner', suffixes=[None, f'_{other_link}'])\n",
    "\n",
    "    model = RUSBoostClassifier_(base_estimator=DecisionTreeClassifier(max_depth=3), n_estimators=60, learning_rate=0.3).fit(training_df_link, \"tt_ar[^-]+$\", 'LOS_dep')\n",
    "    joblib.dump(model, filename)\n",
    "\n",
    "    for LOS in range(1,7):\n",
    "        validation_df_link = validation_df[(validation_df['link_id'] == LINK) & (validation_df['LOS_dep'] == LOS)].copy()\n",
    "        for other_link in other_links:\n",
    "            validation_df_link = pd.merge(validation_df_link, validation_df[validation_df['link_id'] == other_link].filter(regex=(\"tt_ar[^-]+$\")), on=['date'], how='inner', suffixes=[None, f'_{other_link}'])\n",
    "        X_test = validation_df_link.filter(regex=(\"tt_ar[^-]+$\")).values\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        score = metrics.recall_score([LOS]*len(y_pred), y_pred, average='micro')\n",
    "        print(f\"\\tLOS {LOS}: Recall {round(score*100, 2) }%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sOP6-k1pNP3l",
    "tags": []
   },
   "source": [
    "### **FEATURE ENGINEERING ADJACENT LINK INFORMATION + TEMPORAL INFORMATION (UP TO PREVIOUS 45MIN)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rxNerGt4wTQW"
   },
   "source": [
    "*Features creation*\n",
    "\n",
    "For each link, the tt_arrival of the other links, covering the previous 50min from prediction time, are added as features. Therefore we will be using 60 features in total as maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6JecW2nvBMv",
    "outputId": "dc442871-3613-4bf2-f223-8081041cbfc5"
   },
   "outputs": [],
   "source": [
    "print(f'USING PREVIOUS {INPUT_MINUTES_BEFORE}m INFORMATION')\n",
    "for LINK in range(5,11):\n",
    "    print(f\"LINK {LINK}\")\n",
    "    filename = f'{ROOT}/models/RUSBoost_{PREDICTED_MINUTES_AHEAD}m_SAMME_temp{INPUT_MINUTES_BEFORE}m_link{LINK}.joblib'\n",
    "\n",
    "    other_links = [5,6,7,8,9,10]; other_links.remove(LINK)\n",
    "    training_df_link = training_df.copy()\n",
    "    training_df_link = training_df_link[training_df_link['link_id'] == LINK]\n",
    "\n",
    "    for other_link in other_links:\n",
    "        training_df_link = pd.merge(training_df_link, training_df[training_df['link_id'] == other_link].filter(regex=(\"tt_arr.*\")), on=['date'], how='inner', suffixes=[None, f'_{other_link}'])\n",
    "\n",
    "    model = RUSBoostClassifier_(base_estimator=DecisionTreeClassifier(max_depth=3), n_estimators=60, learning_rate=0.3).fit(training_df_link, \"tt_arr.*\", 'LOS_dep')\n",
    "    joblib.dump(model, filename)\n",
    "\n",
    "    for LOS in range(1,7):\n",
    "        validation_df_link = validation_df[(validation_df['link_id'] == LINK) & (validation_df['LOS_dep'] == LOS)].copy()\n",
    "        for other_link in other_links:\n",
    "            validation_df_link = pd.merge(validation_df_link, validation_df[validation_df['link_id'] == other_link].filter(regex=(\"tt_arr.*\")), on=['date'], how='inner', suffixes=[None, f'_{other_link}'])\n",
    "        X_test = validation_df_link.filter(regex=(\"tt_arr.*\")).values\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        score = metrics.recall_score([LOS]*len(y_pred), y_pred, average='micro')\n",
    "        print(f\"\\tLOS {LOS}: Recall {round(score*100, 2) }%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OOgz5KTvr6jU"
   },
   "source": [
    "## Feature engineering M2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mc6_lQUSsKiM",
    "tags": []
   },
   "source": [
    "### **FEATURE ENGINEERING ADJACENT LINK INFORMATION**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_c5Zd21qsNCy"
   },
   "source": [
    "*Features creation*\n",
    "\n",
    "For each link, the tt_arrival of the other links are added as features. Therefore we will be using 6 features in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKswQdsJsPLO"
   },
   "outputs": [],
   "source": [
    "for LINK in range(5,11):\n",
    "    print(f\"LINK {LINK}\")\n",
    "    filename = f'{ROOT}/models/RUSBoost_{PREDICTED_MINUTES_AHEAD}m_M2_adjacent_link{LINK}.joblib'\n",
    "\n",
    "    other_links = [5,6,7,8,9,10]; other_links.remove(LINK)\n",
    "    training_df_link = training_df.copy()\n",
    "    training_df_link = training_df_link[training_df_link['link_id'] == LINK]\n",
    "\n",
    "    for other_link in other_links:\n",
    "        training_df_link = pd.merge(training_df_link, training_df[training_df['link_id'] == other_link].filter(regex=(\"tt_ar[^-]+$\")), on=['date'], how='inner', suffixes=[None, f'_{other_link}'])\n",
    "\n",
    "    model = RUSBoostClassifier_(base_estimator=DecisionTreeClassifier(max_depth=3), n_estimators=60, learning_rate=0.3).fit(training_df_link, \"tt_ar[^-]+$\", 'LOS_dep')\n",
    "    joblib.dump(model, filename)\n",
    "\n",
    "    for LOS in range(1,7):\n",
    "        validation_df_link = validation_df[(validation_df['link_id'] == LINK) & (validation_df['LOS_dep'] == LOS)].copy()\n",
    "        for other_link in other_links:\n",
    "            validation_df_link = pd.merge(validation_df_link, validation_df[validation_df['link_id'] == other_link].filter(regex=(\"tt_ar[^-]+$\")), on=['date'], how='inner', suffixes=[None, f'_{other_link}'])\n",
    "        X_test = validation_df_link.filter(regex=(\"tt_ar[^-]+$\")).values\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        score = metrics.recall_score([LOS]*len(y_pred), y_pred, average='micro')\n",
    "        print(f\"\\tLOS {LOS}: Recall {round(score*100, 2) }%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "poTwjhkKsQ62",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **FEATURE ENGINEERING ADJACENT LINK INFORMATION + TEMPORAL INFORMATION (UP TO PREVIOUS 45MIN)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LFmqR-o_sTPj"
   },
   "source": [
    "*Features creation*\n",
    "\n",
    "For each link, the tt_arrival of the other links, covering the previous 50min from prediction time, are added as features. Therefore we will be using 60 features in total as maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y74FuDnRsVhu",
    "outputId": "11f55f1e-2ccd-44f5-c815-181251dcff05"
   },
   "outputs": [],
   "source": [
    "print(f'USING PREVIOUS {INPUT_MINUTES_BEFORE}m INFORMATION')\n",
    "for LINK in range(5,11):\n",
    "    print(f\"LINK {LINK}\")\n",
    "    filename = f'{ROOT}/models/RUSBoost_{PREDICTED_MINUTES_AHEAD}m_M2_temp{INPUT_MINUTES_BEFORE}m_link{LINK}.joblib'\n",
    "\n",
    "    other_links = [5,6,7,8,9,10]; other_links.remove(LINK)\n",
    "    training_df_link = training_df.copy()\n",
    "    training_df_link = training_df_link[training_df_link['link_id'] == LINK]\n",
    "\n",
    "    for other_link in other_links:\n",
    "        training_df_link = pd.merge(training_df_link, training_df[training_df['link_id'] == other_link].filter(regex=(\"tt_arr.*\")), on=['date'], how='inner', suffixes=[None, f'_{other_link}'])\n",
    "\n",
    "    model = RUSBoostClassifier_(base_estimator=DecisionTreeClassifier(max_depth=3), n_estimators=60, learning_rate=0.3).fit(training_df_link, \"tt_arr.*\", 'LOS_dep')\n",
    "    joblib.dump(model, filename)\n",
    "\n",
    "    for LOS in range(1,7):\n",
    "        validation_df_link = validation_df[(validation_df['link_id'] == LINK) & (validation_df['LOS_dep'] == LOS)].copy()\n",
    "        for other_link in other_links:\n",
    "            validation_df_link = pd.merge(validation_df_link, validation_df[validation_df['link_id'] == other_link].filter(regex=(\"tt_arr.*\")), on=['date'], how='inner', suffixes=[None, f'_{other_link}'])\n",
    "        X_test = validation_df_link.filter(regex=(\"tt_arr.*\")).values\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        score = metrics.recall_score([LOS]*len(y_pred), y_pred, average='micro')\n",
    "        print(f\"\\tLOS {LOS}: Recall {round(score*100, 2) }%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMHe0NkQoa9ACIkTbFtjTmK",
   "include_colab_link": true,
   "mount_file_id": "1Z2Z4EeQPbfgdwWew4IfwOLqQ_conuu2c",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
